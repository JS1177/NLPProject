{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "VAR = {\n",
    "    'max_len': 128,\n",
    "    'batch_size': 64\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:13.362887Z",
     "start_time": "2024-02-17T13:21:13.347498800Z"
    }
   },
   "id": "bfd2aed3b089c3cd",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JS\\AppData\\Local\\Temp\\ipykernel_13640\\4211774795.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\JS\\anaconda3\\envs\\NLPProjectVenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, AdamW, BertForSequenceClassification\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:17.650955Z",
     "start_time": "2024-02-17T13:21:13.351887700Z"
    }
   },
   "id": "5754cad4179ffa39",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification Task: Initiating Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bdcc74464b9a543"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from preprocessing_NLP import pipeline\n",
    "\n",
    "resume_df = pipeline('UpdatedResumeDataSet_T1_7.csv', feature_name='Resume')\n",
    "resume_df = resume_df.reset_index(drop=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:26.021088600Z",
     "start_time": "2024-02-17T13:21:17.651954300Z"
    }
   },
   "id": "c4fcb571da36c413",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def split_into_sub_length(resume: str, max_len=128):\n",
    "    # Ensures all words are tokenized and analyzed;  Due to keras pad_sequences()\n",
    "    resume_list = resume.split()\n",
    "    \n",
    "    resume_length = len(resume_list)\n",
    "    \n",
    "    splits = resume_length // max_len\n",
    "    remaining_splits = resume_length % max_len\n",
    "    \n",
    "    resume_at_max_len = []\n",
    "\n",
    "    for i in range(splits):\n",
    "        resume_section = ' '.join(resume_list[i*max_len: (i+1)*max_len])\n",
    "        # print(resume_list[i*max_len: (i+1)*max_len])\n",
    "        resume_at_max_len.append(resume_section)\n",
    "        \n",
    "    final_section = ' '.join(resume_list[-1-remaining_splits:-1])\n",
    "    resume_at_max_len.append(final_section)\n",
    "    \n",
    "    return resume_at_max_len"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:26.027295100Z",
     "start_time": "2024-02-17T13:21:26.023595400Z"
    }
   },
   "id": "e2aeb3a64100666",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0      [qwtnrvduof education detail may 2013 may 2017...\n1      [qwtnrvduof area interest deep learn control s...\n2      [skill r python sap hana tableau sap hana sql ...\n3      [education detail mca ymcaust faridabad haryan...\n4      [skill c basic iot python matlab data science ...\n                             ...                        \n183    [skill set o window xp 7 8 8bntgbqlmkk1 10 dat...\n184    [good logical analytical skill positive attitu...\n185    [personal skill quick learner eagerness learn ...\n186    [core skill project program management agile s...\n187    [education detail february 2006 february 2006 ...\nName: Resume, Length: 188, dtype: object"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_df['Resume'] = resume_df['Resume'].apply(split_into_sub_length)\n",
    "resume_df['Resume']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:26.036332100Z",
     "start_time": "2024-02-17T13:21:26.026294100Z"
    }
   },
   "id": "ecc91aaaa708a270",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "             Category                                             Resume\n0        Data Science  qwtnrvduof education detail may 2013 may 2017 ...\n0        Data Science                   mixed attribute company matelabs\n1        Data Science  qwtnrvduof area interest deep learn control sy...\n1        Data Science  year month mathematics exprience less 1 year m...\n2        Data Science  skill r python sap hana tableau sap hana sql s...\n..                ...                                                ...\n186   DevOps Engineer  various type test like system regression sanit...\n186   DevOps Engineer  transition drill session forward reverse shado...\n186   DevOps Engineer  action plan team prepare implement business co...\n187  Business Analyst  education detail february 2006 february 2006 t...\n187  Business Analyst  manager sriqmadshiyn manger responsibility han...\n\n[520 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Category</th>\n      <th>Resume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Data Science</td>\n      <td>qwtnrvduof education detail may 2013 may 2017 ...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Data Science</td>\n      <td>mixed attribute company matelabs</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Data Science</td>\n      <td>qwtnrvduof area interest deep learn control sy...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Data Science</td>\n      <td>year month mathematics exprience less 1 year m...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Data Science</td>\n      <td>skill r python sap hana tableau sap hana sql s...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>DevOps Engineer</td>\n      <td>various type test like system regression sanit...</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>DevOps Engineer</td>\n      <td>transition drill session forward reverse shado...</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>DevOps Engineer</td>\n      <td>action plan team prepare implement business co...</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>Business Analyst</td>\n      <td>education detail february 2006 february 2006 t...</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>Business Analyst</td>\n      <td>manager sriqmadshiyn manger responsibility han...</td>\n    </tr>\n  </tbody>\n</table>\n<p>520 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_df = resume_df.explode('Resume')\n",
    "resume_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:26.066078100Z",
     "start_time": "2024-02-17T13:21:26.036332100Z"
    }
   },
   "id": "b84583baa367d2c8",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "str"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resume_df['Category'].iloc[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:26.067085900Z",
     "start_time": "2024-02-17T13:21:26.043968Z"
    }
   },
   "id": "5c8a4fc552d2e32",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:26.067085900Z",
     "start_time": "2024-02-17T13:21:26.048086600Z"
    }
   },
   "id": "400fc2cad34a0507",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "resume_df['Category'] = encoder.fit_transform(resume_df['Category'])\n",
    "resume_df['Category'] = resume_df['Category'].astype(numpy.int64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:26.068085500Z",
     "start_time": "2024-02-17T13:21:26.051271500Z"
    }
   },
   "id": "8e0163d8e40ed2bc",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.int64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resume_df['Category'].iloc[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:26.069085Z",
     "start_time": "2024-02-17T13:21:26.054680900Z"
    }
   },
   "id": "e11bbbbfcb2ccf",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "41"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_df['Category'].value_counts().max()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:26.079636900Z",
     "start_time": "2024-02-17T13:21:26.058825100Z"
    }
   },
   "id": "2cc985a272acc3ac",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# minimum = resume_df['Category'].value_counts().max()\n",
    "minimum = 0\n",
    "current_lowest = resume_df['Category'].value_counts().min()\n",
    "\n",
    "#Check\n",
    "count = resume_df['Category'].value_counts()\n",
    "remaining = 7 - count[count<minimum]\n",
    "\n",
    "while len(remaining != 0):\n",
    "    count = resume_df['Category'].value_counts()\n",
    "    remaining = 7 - count[count<minimum]\n",
    "\n",
    "    for category in remaining.index:\n",
    "        someInt = random.randint(0, current_lowest-1)\n",
    "        value_to_append = resume_df[\n",
    "            resume_df['Category']==category\n",
    "            ]['Resume'].values[someInt]\n",
    "\n",
    "\n",
    "        df_to_concat = pandas.DataFrame({\n",
    "            'Category': [category],\n",
    "            'Resume': [value_to_append]\n",
    "        })\n",
    "\n",
    "        resume_df = pandas.concat([resume_df, df_to_concat], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:26.080636700Z",
     "start_time": "2024-02-17T13:21:26.064119200Z"
    }
   },
   "id": "c99a26a2dd5ba6d8",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Category\n7     41\n18    39\n6     35\n8     31\n15    27\n4     27\n13    25\n2     25\n21    23\n9     22\n17    21\n5     18\n10    17\n12    16\n19    16\n1     15\n11    15\n16    15\n0     14\n23    14\n14    14\n20    14\n3     13\n24    12\n22    11\nName: count, dtype: int64"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_df['Category'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:26.082635500Z",
     "start_time": "2024-02-17T13:21:26.068085500Z"
    }
   },
   "id": "57e00629fa47967e",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "resume_df = resume_df.reset_index(drop=True)\n",
    "test_df = resume_df.sample(100, random_state=42)\n",
    "resume_df = resume_df.drop(test_df.index)\n",
    "\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "resume_df = resume_df.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:26.128765500Z",
     "start_time": "2024-02-17T13:21:26.072232500Z"
    }
   },
   "id": "7ba7da796b960319",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Augmentation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "122abdb2d81a07df"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import nlpaug.flow as naf\n",
    "import nlpaug.augmenter.word as naw\n",
    "from nlpaug.util.file.download import DownloadUtil\n",
    "# DownloadUtil.download_word2vec(dest_dir='.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:26.223327400Z",
     "start_time": "2024-02-17T13:21:26.076638700Z"
    }
   },
   "id": "e3275e73b978758a",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "flow = naf.Sequential([\n",
    "    # naw.WordEmbsAug(model_type='word2vec', model_path='GoogleNews-vectors-negative300.bin',\n",
    "    #                 action=\"substitute\", aug_p=0.2),\n",
    "    # naw.WordEmbsAug(model_type='word2vec', model_path='GoogleNews-vectors-negative300.bin',\n",
    "    #                 action=\"insert\", aug_p=0.1),\n",
    "    # naw.RandomWordAug(action=\"swap\", aug_p=0.5),\n",
    "    # naw.RandomWordAug(action=\"delete\", aug_p=0.1),\n",
    "    \n",
    "    naw.SynonymAug(aug_src='wordnet', aug_p=0.3)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:26.271571900Z",
     "start_time": "2024-02-17T13:21:26.224327800Z"
    }
   },
   "id": "85e131676f39c129",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "resume_df['Resume'] = resume_df['Resume'].apply(lambda x: flow.augment(x, n=3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:30.789153600Z",
     "start_time": "2024-02-17T13:21:26.273575300Z"
    }
   },
   "id": "b3fc8f9b4c5d3ecb",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "resume_df = resume_df.explode('Resume')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:30.796233800Z",
     "start_time": "2024-02-17T13:21:30.791154800Z"
    }
   },
   "id": "3564c0dd95b6ad27",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     Category                                             Resume\n0           6                   mixed dimension company matelabs\n0           6           miscellaneous attribute company matelabs\n0           6            miscellaneous attribute caller matelabs\n1           6  year month mathematics exprience le 1 twelvemo...\n1           6  year month mathematics exprience less one year...\n..        ...                                                ...\n418         8  action program team prepare implement business...\n418         8  activity plan squad prepare implement business...\n419         4  education detail february 2006 february 2006 t...\n419         4  education detail february 2006 february 2006 t...\n419         4  education detail february 2006 february 2006 t...\n\n[1254 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Category</th>\n      <th>Resume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>mixed dimension company matelabs</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>miscellaneous attribute company matelabs</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>miscellaneous attribute caller matelabs</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>year month mathematics exprience le 1 twelvemo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>year month mathematics exprience less one year...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>418</th>\n      <td>8</td>\n      <td>action program team prepare implement business...</td>\n    </tr>\n    <tr>\n      <th>418</th>\n      <td>8</td>\n      <td>activity plan squad prepare implement business...</td>\n    </tr>\n    <tr>\n      <th>419</th>\n      <td>4</td>\n      <td>education detail february 2006 february 2006 t...</td>\n    </tr>\n    <tr>\n      <th>419</th>\n      <td>4</td>\n      <td>education detail february 2006 february 2006 t...</td>\n    </tr>\n    <tr>\n      <th>419</th>\n      <td>4</td>\n      <td>education detail february 2006 february 2006 t...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1254 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_df.dropna(inplace=True)\n",
    "\n",
    "resume_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:30.803990Z",
     "start_time": "2024-02-17T13:21:30.796233800Z"
    }
   },
   "id": "2b275002e991271b",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "resumes = test_df.Resume.values\n",
    "resumes = [\"[CLS] \" + resume + \" [SEP]\" for resume in resumes]\n",
    "testing_labels = test_df.Category.values\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = [tokenizer.tokenize(resume) for resume in resumes]\n",
    "\n",
    "testing_inputs = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "testing_inputs = pad_sequences(testing_inputs, maxlen=VAR['max_len'], dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "testing_masks = []\n",
    "for sequence in testing_inputs:\n",
    "    sequence_mask = [float(id > 0) for id in sequence]\n",
    "    testing_masks.append(sequence_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:31.437822800Z",
     "start_time": "2024-02-17T13:21:30.803482400Z"
    }
   },
   "id": "59fc5eced3a21a2c",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "resumes = resume_df.Resume.values\n",
    "resumes = [\"[CLS] \" + resume + \" [SEP]\" for resume in resumes]\n",
    "labels = resume_df.Category.values\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = [tokenizer.tokenize(resume) for resume in resumes]\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=VAR['max_len'], dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks = []\n",
    "for sequence in input_ids:\n",
    "    sequence_mask = [float(id > 0) for id in sequence]\n",
    "    attention_masks.append(sequence_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:33.254528300Z",
     "start_time": "2024-02-17T13:21:31.439822900Z"
    }
   },
   "id": "a3006610ca4fe1d4",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:33.262030400Z",
     "start_time": "2024-02-17T13:21:33.255519300Z"
    }
   },
   "id": "e0bf5e4277f22d43",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_inputs, validation_inputs, training_labels, validation_labels, training_masks, validation_masks = train_test_split(\n",
    "    input_ids, labels, attention_masks,\n",
    "    random_state=42, test_size=0.3\n",
    ")\n",
    "\n",
    "training_data = TensorDataset(torch.tensor(training_inputs), torch.tensor(training_masks), torch.tensor(training_labels))\n",
    "training_sampler = RandomSampler(training_data)\n",
    "training_dataloader = DataLoader(training_data, sampler=training_sampler, batch_size=VAR['batch_size'])\n",
    "\n",
    "validation_data = TensorDataset(torch.tensor(validation_inputs), torch.tensor(validation_masks), torch.tensor(validation_labels))\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=VAR['batch_size'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:33.278039200Z",
     "start_time": "2024-02-17T13:21:33.259028800Z"
    }
   },
   "id": "658924c0d6ad7318",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\JS\\anaconda3\\envs\\NLPProjectVenv\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "configuration = BertConfig()\n",
    "model = BertModel(configuration)\n",
    "configuration = model.config\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(encoder.classes_))\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:35.236532100Z",
     "start_time": "2024-02-17T13:21:33.279038400Z"
    }
   },
   "id": "9ad5ca72f994a3c2",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def flat_accuracy(predicted_labels, labels):\n",
    "    predicted_labels = numpy.argmax(predicted_labels.to('cpu').numpy(), axis=1).flatten()\n",
    "    labels = labels.to('cpu').numpy().flatten()\n",
    "    return numpy.sum(predicted_labels == labels) / len(labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:21:35.242996500Z",
     "start_time": "2024-02-17T13:21:35.238488800Z"
    }
   },
   "id": "1913e5055f49bdb7",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Training Loss: 3.1998044763292586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|â–ˆ         | 1/10 [00:05<00:51,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: 0.09827302631578948\n",
      "Epoch 2: Average Training Loss: 2.985216804913112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|â–ˆâ–ˆ        | 2/10 [00:11<00:45,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Accuracy: 0.1695449561403509\n",
      "Epoch 3: Average Training Loss: 2.6669307436261858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:16<00:39,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Validation Accuracy: 0.23789290935672514\n",
      "Epoch 4: Average Training Loss: 2.3292363711765836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:22<00:33,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Validation Accuracy: 0.35667945906432746\n",
      "Epoch 5: Average Training Loss: 1.9099662474223547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:28<00:28,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Validation Accuracy: 0.4822733918128655\n",
      "Epoch 6: Average Training Loss: 1.546489179134369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:33<00:22,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Validation Accuracy: 0.6355537280701754\n",
      "Epoch 7: Average Training Loss: 1.205161018030984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:39<00:16,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Validation Accuracy: 0.7562591374269005\n",
      "Epoch 8: Average Training Loss: 0.8848496462617602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:44<00:11,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Validation Accuracy: 0.8529331140350878\n",
      "Epoch 9: Average Training Loss: 0.6292311506611961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:50<00:05,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Validation Accuracy: 0.9280884502923977\n",
      "Epoch 10: Average Training Loss: 0.3987915962934494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:56<00:00,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Validation Accuracy: 0.9352613304093568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "epochs = 10\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "training_losses = []\n",
    "\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    training_steps = 0\n",
    "\n",
    "    for step, batch in enumerate(training_dataloader):\n",
    "        inputs = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        # labels = labels.type(torch.LongTensor)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_masks, labels=labels)\n",
    "        # print(\"TRAIN\", outputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss += loss.item()\n",
    "        training_steps += 1\n",
    "\n",
    "        training_losses.append(loss.item())\n",
    "\n",
    "    average_training_loss = training_loss/training_steps\n",
    "    print(\"Epoch {}: Average Training Loss: {}\".format(epoch+1, average_training_loss))\n",
    "\n",
    "    model.eval()\n",
    "    validation_accuracy = 0\n",
    "    validation_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        inputs = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, attention_mask=attention_masks, labels=labels)\n",
    "            # print(\"VAL\", outputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        temp_validation_accuracy = flat_accuracy(logits, labels)\n",
    "        validation_accuracy += temp_validation_accuracy\n",
    "        validation_steps += 1\n",
    "\n",
    "    average_validation_accuracy = validation_accuracy/validation_steps\n",
    "    print(\"Epoch {}: Validation Accuracy: {}\".format(epoch+1, average_validation_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:22:31.335309800Z",
     "start_time": "2024-02-17T13:21:35.242996500Z"
    }
   },
   "id": "4f600ad255b3cc31",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(testing_inputs)\n",
    "attention_masks = torch.tensor(testing_masks)\n",
    "labels = torch.tensor(testing_labels)\n",
    "\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_dataloader = DataLoader(prediction_data, batch_size=VAR['batch_size'])\n",
    "\n",
    "model.eval()\n",
    "logits_set = []\n",
    "labels_set = []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "    batch_input_ids, batch_attention_masks, batch_labels = batch\n",
    "    batch_input_ids, batch_attention_masks, batch_labels = batch_input_ids.to(device), batch_attention_masks.to(device), batch_labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_attention_masks)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    logits_set.append(logits.cpu().numpy())\n",
    "    labels_set.append(batch_labels.cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:22:31.529591600Z",
     "start_time": "2024-02-17T13:22:31.337310400Z"
    }
   },
   "id": "bd469554800788e0",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: MCC = 0.6412299198109214\n",
      "Batch 2: MCC = 0.7435707691490477\n",
      "\n",
      "Overall MCC: 0.6924003444799846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_set = []\n",
    "\n",
    "# Calculate Matthews correlation coefficient for each batch\n",
    "for i in range(len(labels_set)):\n",
    "    mcc = matthews_corrcoef(labels_set[i], numpy.argmax(logits_set[i], axis=1).flatten())\n",
    "    matthews_set.append(mcc)\n",
    "\n",
    "for i, mcc in enumerate(matthews_set):\n",
    "    print(f\"Batch {i + 1}: MCC = {mcc}\")\n",
    "\n",
    "# Calculate the overall Matthews correlation coefficient\n",
    "overall_mcc = numpy.mean(matthews_set)\n",
    "print(f\"\\nOverall MCC: {overall_mcc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:22:31.535400800Z",
     "start_time": "2024-02-17T13:22:31.531593500Z"
    }
   },
   "id": "25069398e8ed20d4",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_accuracy=0.703125, mean_precision=0.7702763310185186, mean_recall=0.703125, mean_f1=0.6946201849847684\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "\n",
    "for i in range(len(labels_set)):\n",
    "    acc = accuracy_score(labels_set[i], numpy.argmax(logits_set[i], axis=1).flatten())\n",
    "    accuracy_scores.append(acc)\n",
    "    \n",
    "    precision = precision_score(labels_set[i], numpy.argmax(logits_set[i], axis=1).flatten(), average='weighted', zero_division=0)\n",
    "    precision_scores.append(precision)\n",
    "    \n",
    "    recall = recall_score(labels_set[i], numpy.argmax(logits_set[i], axis=1).flatten(), average='weighted', zero_division=0)\n",
    "    recall_scores.append(recall)\n",
    "    \n",
    "    f1_scoring = f1_score(labels_set[i], numpy.argmax(logits_set[i], axis=1).flatten(), average='weighted', zero_division=0)\n",
    "    f1_scores.append(f1_scoring)\n",
    "    \n",
    "mean_accuracy = numpy.mean(accuracy_scores)\n",
    "mean_precision = numpy.mean(precision_scores)\n",
    "mean_recall = numpy.mean(recall_scores)\n",
    "mean_f1 = numpy.mean(f1_scores)\n",
    "\n",
    "print(f'{mean_accuracy=}, {mean_precision=}, {mean_recall=}, {mean_f1=}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:22:31.546652900Z",
     "start_time": "2024-02-17T13:22:31.537400Z"
    }
   },
   "id": "e14d894193322ef9",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T13:22:31.548656700Z",
     "start_time": "2024-02-17T13:22:31.546150400Z"
    }
   },
   "id": "d6bb7649cd834cd9",
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
