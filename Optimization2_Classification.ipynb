{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "VAR = {\n",
    "    'max_len': 512,\n",
    "    'batch_size': 16\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:44.870485400Z",
     "start_time": "2024-02-18T06:25:44.844789Z"
    }
   },
   "id": "bfd2aed3b089c3cd",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JS\\AppData\\Local\\Temp\\ipykernel_45584\\4211774795.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\JS\\anaconda3\\envs\\NLPProjectVenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, AdamW, BertForSequenceClassification\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:49.243773600Z",
     "start_time": "2024-02-18T06:25:44.849484600Z"
    }
   },
   "id": "5754cad4179ffa39",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification Task: Initiating Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bdcc74464b9a543"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from preprocessing_NLP import pipeline\n",
    "\n",
    "resume_df = pipeline('UpdatedResumeDataSet_T1_7.csv', feature_name='Resume')\n",
    "resume_df = resume_df.reset_index(drop=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:57.685720100Z",
     "start_time": "2024-02-18T06:25:49.244774100Z"
    }
   },
   "id": "c4fcb571da36c413",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def split_into_sub_length(resume: str, max_len=128):\n",
    "    # Ensures all words are tokenized and analyzed;  Due to keras pad_sequences()\n",
    "    resume_list = resume.split()\n",
    "    \n",
    "    resume_length = len(resume_list)\n",
    "    \n",
    "    splits = resume_length // max_len\n",
    "    remaining_splits = resume_length % max_len\n",
    "    \n",
    "    resume_at_max_len = []\n",
    "\n",
    "    for i in range(splits):\n",
    "        resume_section = ' '.join(resume_list[i*max_len: (i+1)*max_len])\n",
    "        # print(resume_list[i*max_len: (i+1)*max_len])\n",
    "        resume_at_max_len.append(resume_section)\n",
    "        \n",
    "    final_section = ' '.join(resume_list[-1-remaining_splits:-1])\n",
    "    resume_at_max_len.append(final_section)\n",
    "    \n",
    "    return resume_at_max_len"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:57.691465500Z",
     "start_time": "2024-02-18T06:25:57.687721400Z"
    }
   },
   "id": "e2aeb3a64100666",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0      [qwtnrvduof education detail may 2013 may 2017...\n1      [qwtnrvduof area interest deep learn control s...\n2      [skill r python sap hana tableau sap hana sql ...\n3      [education detail mca ymcaust faridabad haryan...\n4      [skill c basic iot python matlab data science ...\n                             ...                        \n183    [skill set o window xp 7 8 8bntgbqlmkk1 10 dat...\n184    [good logical analytical skill positive attitu...\n185    [personal skill quick learner eagerness learn ...\n186    [core skill project program management agile s...\n187    [education detail february 2006 february 2006 ...\nName: Resume, Length: 188, dtype: object"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_df['Resume'] = resume_df['Resume'].apply(split_into_sub_length)\n",
    "resume_df['Resume']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:57.702303300Z",
     "start_time": "2024-02-18T06:25:57.690468400Z"
    }
   },
   "id": "ecc91aaaa708a270",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "             Category                                             Resume\n0        Data Science  qwtnrvduof education detail may 2013 may 2017 ...\n0        Data Science                   mixed attribute company matelabs\n1        Data Science  qwtnrvduof area interest deep learn control sy...\n1        Data Science  year month mathematics exprience less 1 year m...\n2        Data Science  skill r python sap hana tableau sap hana sql s...\n..                ...                                                ...\n186   DevOps Engineer  various type test like system regression sanit...\n186   DevOps Engineer  transition drill session forward reverse shado...\n186   DevOps Engineer  action plan team prepare implement business co...\n187  Business Analyst  education detail february 2006 february 2006 t...\n187  Business Analyst  manager sriqmadshiyn manger responsibility han...\n\n[520 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Category</th>\n      <th>Resume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Data Science</td>\n      <td>qwtnrvduof education detail may 2013 may 2017 ...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Data Science</td>\n      <td>mixed attribute company matelabs</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Data Science</td>\n      <td>qwtnrvduof area interest deep learn control sy...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Data Science</td>\n      <td>year month mathematics exprience less 1 year m...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Data Science</td>\n      <td>skill r python sap hana tableau sap hana sql s...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>DevOps Engineer</td>\n      <td>various type test like system regression sanit...</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>DevOps Engineer</td>\n      <td>transition drill session forward reverse shado...</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>DevOps Engineer</td>\n      <td>action plan team prepare implement business co...</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>Business Analyst</td>\n      <td>education detail february 2006 february 2006 t...</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>Business Analyst</td>\n      <td>manager sriqmadshiyn manger responsibility han...</td>\n    </tr>\n  </tbody>\n</table>\n<p>520 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_df = resume_df.explode('Resume')\n",
    "resume_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:57.716705900Z",
     "start_time": "2024-02-18T06:25:57.700304600Z"
    }
   },
   "id": "b84583baa367d2c8",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "str"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resume_df['Category'].iloc[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:57.717713200Z",
     "start_time": "2024-02-18T06:25:57.707456500Z"
    }
   },
   "id": "5c8a4fc552d2e32",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:57.718712900Z",
     "start_time": "2024-02-18T06:25:57.711359800Z"
    }
   },
   "id": "400fc2cad34a0507",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "resume_df['Category'] = encoder.fit_transform(resume_df['Category'])\n",
    "resume_df['Category'] = resume_df['Category'].astype(numpy.int64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:57.731563200Z",
     "start_time": "2024-02-18T06:25:57.714874500Z"
    }
   },
   "id": "8e0163d8e40ed2bc",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.int64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resume_df['Category'].iloc[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:57.733570100Z",
     "start_time": "2024-02-18T06:25:57.718712900Z"
    }
   },
   "id": "e11bbbbfcb2ccf",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "41"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_df['Category'].value_counts().max()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:57.754663500Z",
     "start_time": "2024-02-18T06:25:57.723238900Z"
    }
   },
   "id": "2cc985a272acc3ac",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# minimum = resume_df['Category'].value_counts().max()\n",
    "minimum = 0\n",
    "current_lowest = resume_df['Category'].value_counts().min()\n",
    "\n",
    "#Check\n",
    "count = resume_df['Category'].value_counts()\n",
    "remaining = 7 - count[count<minimum]\n",
    "\n",
    "while len(remaining != 0):\n",
    "    count = resume_df['Category'].value_counts()\n",
    "    remaining = 7 - count[count<minimum]\n",
    "\n",
    "    for category in remaining.index:\n",
    "        someInt = random.randint(0, current_lowest-1)\n",
    "        value_to_append = resume_df[\n",
    "            resume_df['Category']==category\n",
    "            ]['Resume'].values[someInt]\n",
    "\n",
    "\n",
    "        df_to_concat = pandas.DataFrame({\n",
    "            'Category': [category],\n",
    "            'Resume': [value_to_append]\n",
    "        })\n",
    "\n",
    "        resume_df = pandas.concat([resume_df, df_to_concat], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:57.768671700Z",
     "start_time": "2024-02-18T06:25:57.728611200Z"
    }
   },
   "id": "c99a26a2dd5ba6d8",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Category\n7     41\n18    39\n6     35\n8     31\n15    27\n4     27\n13    25\n2     25\n21    23\n9     22\n17    21\n5     18\n10    17\n12    16\n19    16\n1     15\n11    15\n16    15\n0     14\n23    14\n14    14\n20    14\n3     13\n24    12\n22    11\nName: count, dtype: int64"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_df['Category'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:57.798183600Z",
     "start_time": "2024-02-18T06:25:57.732563400Z"
    }
   },
   "id": "57e00629fa47967e",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "resume_df = resume_df.reset_index(drop=True)\n",
    "test_df = resume_df.sample(100, random_state=42)\n",
    "resume_df = resume_df.drop(test_df.index)\n",
    "\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "resume_df = resume_df.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:57.799182300Z",
     "start_time": "2024-02-18T06:25:57.738007100Z"
    }
   },
   "id": "7ba7da796b960319",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Augmentation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "122abdb2d81a07df"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import nlpaug.flow as naf\n",
    "import nlpaug.augmenter.word as naw\n",
    "from nlpaug.util.file.download import DownloadUtil\n",
    "# DownloadUtil.download_word2vec(dest_dir='.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:57.892547900Z",
     "start_time": "2024-02-18T06:25:57.741149100Z"
    }
   },
   "id": "e3275e73b978758a",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "flow = naf.Sequential([\n",
    "    # naw.WordEmbsAug(model_type='word2vec', model_path='GoogleNews-vectors-negative300.bin',\n",
    "    #                 action=\"substitute\", aug_p=0.2),\n",
    "    # naw.WordEmbsAug(model_type='word2vec', model_path='GoogleNews-vectors-negative300.bin',\n",
    "    #                 action=\"insert\", aug_p=0.1),\n",
    "    # naw.RandomWordAug(action=\"swap\", aug_p=0.5),\n",
    "    # naw.RandomWordAug(action=\"delete\", aug_p=0.1),\n",
    "    \n",
    "    naw.SynonymAug(aug_src='wordnet', aug_p=0.3)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:25:57.940866400Z",
     "start_time": "2024-02-18T06:25:57.895547Z"
    }
   },
   "id": "85e131676f39c129",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "resume_df['Resume'] = resume_df['Resume'].apply(lambda x: flow.augment(x, n=3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:26:02.581745200Z",
     "start_time": "2024-02-18T06:25:57.941866200Z"
    }
   },
   "id": "b3fc8f9b4c5d3ecb",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "resume_df = resume_df.explode('Resume')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:26:02.589230600Z",
     "start_time": "2024-02-18T06:26:02.583747900Z"
    }
   },
   "id": "3564c0dd95b6ad27",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     Category                                             Resume\n0           6                   mixed attribute society matelabs\n0           6                      mixed property party matelabs\n0           6                  sundry dimension company matelabs\n1           6  year calendar month mathematics exprience less...\n1           6  year month mathematics exprience less 1 yr mon...\n..        ...                                                ...\n418         8  action plan team prepare implement business co...\n418         8  action plan team prepare implement business co...\n419         4  education detail february 2006 february 2006 t...\n419         4  education detail february 2006 february 2006 t...\n419         4  education detail february 2006 february 2006 t...\n\n[1254 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Category</th>\n      <th>Resume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>mixed attribute society matelabs</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>mixed property party matelabs</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>sundry dimension company matelabs</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>year calendar month mathematics exprience less...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>year month mathematics exprience less 1 yr mon...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>418</th>\n      <td>8</td>\n      <td>action plan team prepare implement business co...</td>\n    </tr>\n    <tr>\n      <th>418</th>\n      <td>8</td>\n      <td>action plan team prepare implement business co...</td>\n    </tr>\n    <tr>\n      <th>419</th>\n      <td>4</td>\n      <td>education detail february 2006 february 2006 t...</td>\n    </tr>\n    <tr>\n      <th>419</th>\n      <td>4</td>\n      <td>education detail february 2006 february 2006 t...</td>\n    </tr>\n    <tr>\n      <th>419</th>\n      <td>4</td>\n      <td>education detail february 2006 february 2006 t...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1254 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_df.dropna(inplace=True)\n",
    "\n",
    "resume_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:26:02.597414Z",
     "start_time": "2024-02-18T06:26:02.588231300Z"
    }
   },
   "id": "2b275002e991271b",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "resumes = test_df.Resume.values\n",
    "resumes = [\"[CLS] \" + resume + \" [SEP]\" for resume in resumes]\n",
    "testing_labels = test_df.Category.values\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = [tokenizer.tokenize(resume) for resume in resumes]\n",
    "\n",
    "testing_inputs = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "testing_inputs = pad_sequences(testing_inputs, maxlen=VAR['max_len'], dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "testing_masks = []\n",
    "for sequence in testing_inputs:\n",
    "    sequence_mask = [float(id > 0) for id in sequence]\n",
    "    testing_masks.append(sequence_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:26:03.213351200Z",
     "start_time": "2024-02-18T06:26:02.595414300Z"
    }
   },
   "id": "59fc5eced3a21a2c",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "resumes = resume_df.Resume.values\n",
    "resumes = [\"[CLS] \" + resume + \" [SEP]\" for resume in resumes]\n",
    "labels = resume_df.Category.values\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = [tokenizer.tokenize(resume) for resume in resumes]\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=VAR['max_len'], dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks = []\n",
    "for sequence in input_ids:\n",
    "    sequence_mask = [float(id > 0) for id in sequence]\n",
    "    attention_masks.append(sequence_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:26:05.116585400Z",
     "start_time": "2024-02-18T06:26:03.216355Z"
    }
   },
   "id": "a3006610ca4fe1d4",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:26:05.123040300Z",
     "start_time": "2024-02-18T06:26:05.118036200Z"
    }
   },
   "id": "e0bf5e4277f22d43",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_inputs, validation_inputs, training_labels, validation_labels, training_masks, validation_masks = train_test_split(\n",
    "    input_ids, labels, attention_masks,\n",
    "    random_state=42, test_size=0.3\n",
    ")\n",
    "\n",
    "training_data = TensorDataset(torch.tensor(training_inputs), torch.tensor(training_masks), torch.tensor(training_labels))\n",
    "training_sampler = RandomSampler(training_data)\n",
    "training_dataloader = DataLoader(training_data, sampler=training_sampler, batch_size=VAR['batch_size'])\n",
    "\n",
    "validation_data = TensorDataset(torch.tensor(validation_inputs), torch.tensor(validation_masks), torch.tensor(validation_labels))\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=VAR['batch_size'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:26:05.170851600Z",
     "start_time": "2024-02-18T06:26:05.121039700Z"
    }
   },
   "id": "658924c0d6ad7318",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\JS\\anaconda3\\envs\\NLPProjectVenv\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "configuration = BertConfig()\n",
    "model = BertModel(configuration)\n",
    "configuration = model.config\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(encoder.classes_))\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:26:06.761744100Z",
     "start_time": "2024-02-18T06:26:05.172852500Z"
    }
   },
   "id": "9ad5ca72f994a3c2",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def flat_accuracy(predicted_labels, labels):\n",
    "    predicted_labels = numpy.argmax(predicted_labels.to('cpu').numpy(), axis=1).flatten()\n",
    "    labels = labels.to('cpu').numpy().flatten()\n",
    "    return numpy.sum(predicted_labels == labels) / len(labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:26:06.768368200Z",
     "start_time": "2024-02-18T06:26:06.763860300Z"
    }
   },
   "id": "1913e5055f49bdb7",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Training Loss: 3.1542600935155694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   3%|▎         | 1/30 [00:38<18:43, 38.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: 0.1634837962962963\n",
      "Epoch 2: Average Training Loss: 2.8433662978085605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   7%|▋         | 2/30 [01:12<16:47, 35.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Accuracy: 0.11979166666666667\n",
      "Epoch 3: Average Training Loss: 2.4487760240381413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 3/30 [02:00<18:36, 41.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Validation Accuracy: 0.32233796296296297\n",
      "Epoch 4: Average Training Loss: 1.8965610764243386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  13%|█▎        | 4/30 [02:40<17:40, 40.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Validation Accuracy: 0.6221064814814815\n",
      "Epoch 5: Average Training Loss: 1.2915438944643194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  17%|█▋        | 5/30 [03:19<16:41, 40.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Validation Accuracy: 0.7693865740740741\n",
      "Epoch 6: Average Training Loss: 0.7282042584635995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 6/30 [03:56<15:38, 39.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Validation Accuracy: 0.9120370370370371\n",
      "Epoch 7: Average Training Loss: 0.35550178560343654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  23%|██▎       | 7/30 [04:32<14:33, 37.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Validation Accuracy: 0.953125\n",
      "Epoch 8: Average Training Loss: 0.1952516203576868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  27%|██▋       | 8/30 [05:07<13:36, 37.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Validation Accuracy: 0.9765625\n",
      "Epoch 9: Average Training Loss: 0.12273299517956647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 9/30 [05:42<12:46, 36.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 10: Average Training Loss: 0.08567740937525575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|███▎      | 10/30 [06:17<12:00, 36.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 11: Average Training Loss: 0.06509290520440449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  37%|███▋      | 11/30 [06:52<11:16, 35.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 12: Average Training Loss: 0.05221716849641366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 12/30 [07:26<10:36, 35.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 13: Average Training Loss: 0.04442546719854528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  43%|████▎     | 13/30 [08:01<09:57, 35.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 14: Average Training Loss: 0.037846471098336304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  47%|████▋     | 14/30 [08:36<09:19, 34.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 15: Average Training Loss: 0.03389476016163826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 15/30 [09:10<08:43, 34.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 16: Average Training Loss: 0.030192505365068262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  53%|█████▎    | 16/30 [09:45<08:07, 34.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 17: Average Training Loss: 0.02711086127568375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  57%|█████▋    | 17/30 [10:20<07:31, 34.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 18: Average Training Loss: 0.024457002566619354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 18/30 [10:54<06:57, 34.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 19: Average Training Loss: 0.022424016486514698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  63%|██████▎   | 19/30 [11:29<06:22, 34.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 20: Average Training Loss: 0.0204796279695901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|██████▋   | 20/30 [12:04<05:47, 34.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 21: Average Training Loss: 0.019008434259078718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 21/30 [12:38<05:12, 34.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 22: Average Training Loss: 0.04560814145952463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  73%|███████▎  | 22/30 [13:13<04:37, 34.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Validation Accuracy: 0.9620949074074074\n",
      "Epoch 23: Average Training Loss: 0.04040144832635468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  77%|███████▋  | 23/30 [13:48<04:02, 34.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 24: Average Training Loss: 0.017954169078306718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 24/30 [14:22<03:27, 34.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 25: Average Training Loss: 0.014853194034235044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  83%|████████▎ | 25/30 [14:57<02:53, 34.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 26: Average Training Loss: 0.013990831476720896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  87%|████████▋ | 26/30 [15:32<02:18, 34.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 27: Average Training Loss: 0.012726686319166963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 27/30 [16:06<01:43, 34.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 28: Average Training Loss: 0.011850683577358722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  93%|█████████▎| 28/30 [16:41<01:09, 34.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 29: Average Training Loss: 0.011214340342716737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  97%|█████████▋| 29/30 [17:16<00:34, 34.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Validation Accuracy: 0.9817708333333334\n",
      "Epoch 30: Average Training Loss: 0.010501454601233655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [17:50<00:00, 35.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Validation Accuracy: 0.9817708333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "epochs = 30\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "training_losses = []\n",
    "\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    training_steps = 0\n",
    "\n",
    "    for step, batch in enumerate(training_dataloader):\n",
    "        inputs = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        # labels = labels.type(torch.LongTensor)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_masks, labels=labels)\n",
    "        # print(\"TRAIN\", outputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss += loss.item()\n",
    "        training_steps += 1\n",
    "\n",
    "        training_losses.append(loss.item())\n",
    "\n",
    "    average_training_loss = training_loss/training_steps\n",
    "    print(\"Epoch {}: Average Training Loss: {}\".format(epoch+1, average_training_loss))\n",
    "\n",
    "    model.eval()\n",
    "    validation_accuracy = 0\n",
    "    validation_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        inputs = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, attention_mask=attention_masks, labels=labels)\n",
    "            # print(\"VAL\", outputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        temp_validation_accuracy = flat_accuracy(logits, labels)\n",
    "        validation_accuracy += temp_validation_accuracy\n",
    "        validation_steps += 1\n",
    "\n",
    "    average_validation_accuracy = validation_accuracy/validation_steps\n",
    "    print(\"Epoch {}: Validation Accuracy: {}\".format(epoch+1, average_validation_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:43:57.560572200Z",
     "start_time": "2024-02-18T06:26:06.768368200Z"
    }
   },
   "id": "4f600ad255b3cc31",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(testing_inputs)\n",
    "attention_masks = torch.tensor(testing_masks)\n",
    "labels = torch.tensor(testing_labels)\n",
    "\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_dataloader = DataLoader(prediction_data, batch_size=VAR['batch_size'])\n",
    "\n",
    "model.eval()\n",
    "logits_set = []\n",
    "labels_set = []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "    batch_input_ids, batch_attention_masks, batch_labels = batch\n",
    "    batch_input_ids, batch_attention_masks, batch_labels = batch_input_ids.to(device), batch_attention_masks.to(device), batch_labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_attention_masks)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    logits_set.append(logits.cpu().numpy())\n",
    "    labels_set.append(batch_labels.cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:43:58.555881300Z",
     "start_time": "2024-02-18T06:43:57.562584600Z"
    }
   },
   "id": "bd469554800788e0",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: MCC = 0.7991452991452992\n",
      "Batch 2: MCC = 0.6154070939562221\n",
      "Batch 3: MCC = 0.731608586908178\n",
      "Batch 4: MCC = 0.6217626379732312\n",
      "Batch 5: MCC = 0.6054359545753111\n",
      "Batch 6: MCC = 0.868421052631579\n",
      "Batch 7: MCC = 0.7302967433402214\n",
      "\n",
      "Overall MCC: 0.7102967669328631\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_set = []\n",
    "\n",
    "# Calculate Matthews correlation coefficient for each batch\n",
    "for i in range(len(labels_set)):\n",
    "    mcc = matthews_corrcoef(labels_set[i], numpy.argmax(logits_set[i], axis=1).flatten())\n",
    "    matthews_set.append(mcc)\n",
    "\n",
    "for i, mcc in enumerate(matthews_set):\n",
    "    print(f\"Batch {i + 1}: MCC = {mcc}\")\n",
    "\n",
    "# Calculate the overall Matthews correlation coefficient\n",
    "overall_mcc = numpy.mean(matthews_set)\n",
    "print(f\"\\nOverall MCC: {overall_mcc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:43:58.565835800Z",
     "start_time": "2024-02-18T06:43:58.556881400Z"
    }
   },
   "id": "25069398e8ed20d4",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_accuracy=0.7232142857142857, mean_precision=0.769345238095238, mean_recall=0.7232142857142857, mean_f1=0.7217261904761905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "\n",
    "for i in range(len(labels_set)):\n",
    "    acc = accuracy_score(labels_set[i], numpy.argmax(logits_set[i], axis=1).flatten())\n",
    "    accuracy_scores.append(acc)\n",
    "    \n",
    "    precision = precision_score(labels_set[i], numpy.argmax(logits_set[i], axis=1).flatten(), average='weighted', zero_division=0)\n",
    "    precision_scores.append(precision)\n",
    "    \n",
    "    recall = recall_score(labels_set[i], numpy.argmax(logits_set[i], axis=1).flatten(), average='weighted', zero_division=0)\n",
    "    recall_scores.append(recall)\n",
    "    \n",
    "    f1_scoring = f1_score(labels_set[i], numpy.argmax(logits_set[i], axis=1).flatten(), average='weighted', zero_division=0)\n",
    "    f1_scores.append(f1_scoring)\n",
    "    \n",
    "mean_accuracy = numpy.mean(accuracy_scores)\n",
    "mean_precision = numpy.mean(precision_scores)\n",
    "mean_recall = numpy.mean(recall_scores)\n",
    "mean_f1 = numpy.mean(f1_scores)\n",
    "\n",
    "print(f'{mean_accuracy=}, {mean_precision=}, {mean_recall=}, {mean_f1=}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:43:58.593915700Z",
     "start_time": "2024-02-18T06:43:58.567836800Z"
    }
   },
   "id": "e14d894193322ef9",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:43:58.593915700Z",
     "start_time": "2024-02-18T06:43:58.585912300Z"
    }
   },
   "id": "d6bb7649cd834cd9",
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
