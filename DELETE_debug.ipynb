{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Classification Task: Initiating Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "630f146353fd6177"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jscho\\AppData\\Local\\Temp\\ipykernel_8812\\3917502893.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "def generate_class_labels(labels):\n",
    "\n",
    "    unique_labels = list(set(labels))\n",
    "\n",
    "    unique_labels_map = {}\n",
    "\n",
    "    for index, label in enumerate(unique_labels, start=0):\n",
    "        unique_labels_map[label] = index\n",
    "\n",
    "    return unique_labels_map\n",
    "\n",
    "data_frame = pandas.read_csv(\"cleanedResumes.csv\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "data_frame.columns = 'label', 'sentence'\n",
    "\n",
    "labels = data_frame['label']\n",
    "label_map = generate_class_labels(labels)\n",
    "data_frame['label'] = data_frame['label'].apply(lambda label_name: label_map[label_name])\n",
    "\n",
    "data_frame.sample(10)\n",
    "\n",
    "# Prepare sentences and labels\n",
    "sentences = data_frame.sentence.values\n",
    "# sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = data_frame.label.values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T01:32:59.568187100Z",
     "start_time": "2024-01-29T01:32:57.339502600Z"
    }
   },
   "id": "c40657895e78b536",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     label                                           sentence\n0        4  qwtnrvduof education detail may 2013 to may 20...\n1        4  qwtnrvduof area of interest deep learn control...\n2        4  skill r python sap hana tableau sap hana sql s...\n3        4  education detail mca ymcaust faridabad haryana...\n4        4  skill c basic iot python matlab data science m...\n..     ...                                                ...\n183     23  skill set o window xp 7 8 8bntgbqlmkk1 10 data...\n184     23  good logical and analytical skill positive att...\n185     23  personal skill quick learner eagerness to lear...\n186     13  core skill project program management agile sc...\n187     17  education detail february 2006 to february 200...\n\n[188 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>qwtnrvduof education detail may 2013 to may 20...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>qwtnrvduof area of interest deep learn control...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>skill r python sap hana tableau sap hana sql s...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>education detail mca ymcaust faridabad haryana...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>skill c basic iot python matlab data science m...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>23</td>\n      <td>skill set o window xp 7 8 8bntgbqlmkk1 10 data...</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>23</td>\n      <td>good logical and analytical skill positive att...</td>\n    </tr>\n    <tr>\n      <th>185</th>\n      <td>23</td>\n      <td>personal skill quick learner eagerness to lear...</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>13</td>\n      <td>core skill project program management agile sc...</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>17</td>\n      <td>education detail february 2006 to february 200...</td>\n    </tr>\n  </tbody>\n</table>\n<p>188 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T01:32:59.582674Z",
     "start_time": "2024-01-29T01:32:59.572408300Z"
    }
   },
   "id": "749d4349796a26ad",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jscho\\anaconda3\\envs\\NLPProjectVenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\jscho\\anaconda3\\envs\\NLPProjectVenv\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 1/1000 [00:03<1:04:05,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Training Loss: 3.307697375615438\n",
      "Epoch 1: Validation Accuracy: 0.10526315789473684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 2/1000 [00:05<45:14,  2.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Average Training Loss: 3.208211143811544\n",
      "Epoch 2: Validation Accuracy: 0.05263157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 3/1000 [00:07<39:12,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Average Training Loss: 3.1169822216033936\n",
      "Epoch 3: Validation Accuracy: 0.10526315789473684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 4/1000 [00:09<36:31,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Average Training Loss: 3.1132096449534097\n",
      "Epoch 4: Validation Accuracy: 0.05263157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 5/1000 [00:11<34:59,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Average Training Loss: 2.9384714365005493\n",
      "Epoch 5: Validation Accuracy: 0.05263157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1%|          | 6/1000 [00:13<34:02,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Average Training Loss: 2.8603967825571694\n",
      "Epoch 6: Validation Accuracy: 0.05263157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1%|          | 7/1000 [00:15<33:23,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Average Training Loss: 2.8279240926106772\n",
      "Epoch 7: Validation Accuracy: 0.05263157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1%|          | 8/1000 [00:17<33:01,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Average Training Loss: 2.714881261189779\n",
      "Epoch 8: Validation Accuracy: 0.10526315789473684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1%|          | 9/1000 [00:19<32:45,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Average Training Loss: 2.6096741755803428\n",
      "Epoch 9: Validation Accuracy: 0.05263157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1%|          | 10/1000 [00:21<32:36,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Average Training Loss: 2.476803501447042\n",
      "Epoch 10: Validation Accuracy: 0.15789473684210525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1%|          | 11/1000 [00:23<32:31,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Average Training Loss: 2.4712359507878623\n",
      "Epoch 11: Validation Accuracy: 0.15789473684210525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1%|          | 12/1000 [00:25<32:26,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Average Training Loss: 2.3702671925226846\n",
      "Epoch 12: Validation Accuracy: 0.21052631578947367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1%|▏         | 13/1000 [00:27<32:22,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Average Training Loss: 2.2750934759775796\n",
      "Epoch 13: Validation Accuracy: 0.3157894736842105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   1%|▏         | 14/1000 [00:29<32:20,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Average Training Loss: 2.2302366495132446\n",
      "Epoch 14: Validation Accuracy: 0.10526315789473684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▏         | 15/1000 [00:31<32:19,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Average Training Loss: 2.178211828072866\n",
      "Epoch 15: Validation Accuracy: 0.15789473684210525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▏         | 16/1000 [00:33<32:18,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Average Training Loss: 2.0813910563786826\n",
      "Epoch 16: Validation Accuracy: 0.3157894736842105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▏         | 17/1000 [00:35<32:22,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Average Training Loss: 1.978749672571818\n",
      "Epoch 17: Validation Accuracy: 0.2631578947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▏         | 18/1000 [00:37<32:22,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Average Training Loss: 1.8657977382342021\n",
      "Epoch 18: Validation Accuracy: 0.2631578947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▏         | 19/1000 [00:39<32:18,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Average Training Loss: 1.6414345701535542\n",
      "Epoch 19: Validation Accuracy: 0.3157894736842105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▏         | 20/1000 [00:41<32:19,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Average Training Loss: 1.5263442893822987\n",
      "Epoch 20: Validation Accuracy: 0.3157894736842105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▏         | 21/1000 [00:43<32:17,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Average Training Loss: 1.4431722263495128\n",
      "Epoch 21: Validation Accuracy: 0.3157894736842105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▏         | 22/1000 [00:45<32:18,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Average Training Loss: 1.4663065473238628\n",
      "Epoch 22: Validation Accuracy: 0.2631578947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▏         | 23/1000 [00:47<32:21,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Average Training Loss: 1.2944801648457844\n",
      "Epoch 23: Validation Accuracy: 0.3157894736842105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▏         | 24/1000 [00:49<32:20,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Average Training Loss: 1.2125499447186787\n",
      "Epoch 24: Validation Accuracy: 0.3684210526315789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▎         | 25/1000 [00:51<32:19,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Average Training Loss: 1.181723674138387\n",
      "Epoch 25: Validation Accuracy: 0.3684210526315789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   3%|▎         | 26/1000 [00:53<32:18,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Average Training Loss: 1.0745592415332794\n",
      "Epoch 26: Validation Accuracy: 0.42105263157894735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   3%|▎         | 27/1000 [00:55<32:16,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Average Training Loss: 1.0695224304993947\n",
      "Epoch 27: Validation Accuracy: 0.47368421052631576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   3%|▎         | 28/1000 [00:57<32:18,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Average Training Loss: 0.9554974337418874\n",
      "Epoch 28: Validation Accuracy: 0.5263157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   3%|▎         | 29/1000 [00:59<32:18,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Average Training Loss: 0.9707877536614736\n",
      "Epoch 29: Validation Accuracy: 0.47368421052631576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   3%|▎         | 30/1000 [01:01<32:21,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Average Training Loss: 0.8765203058719635\n",
      "Epoch 30: Validation Accuracy: 0.5263157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   3%|▎         | 31/1000 [01:03<32:22,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Average Training Loss: 0.8320548931757609\n",
      "Epoch 31: Validation Accuracy: 0.5263157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   3%|▎         | 32/1000 [01:05<32:27,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Average Training Loss: 0.8874687453111013\n",
      "Epoch 32: Validation Accuracy: 0.5263157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   3%|▎         | 33/1000 [01:07<32:28,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Average Training Loss: 0.760360966126124\n",
      "Epoch 33: Validation Accuracy: 0.5789473684210527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   3%|▎         | 34/1000 [01:09<32:27,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Average Training Loss: 0.7564536233743032\n",
      "Epoch 34: Validation Accuracy: 0.47368421052631576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   4%|▎         | 35/1000 [01:11<32:29,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Average Training Loss: 0.6813815186421076\n",
      "Epoch 35: Validation Accuracy: 0.42105263157894735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   4%|▎         | 36/1000 [01:13<32:27,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Average Training Loss: 0.6270691603422165\n",
      "Epoch 36: Validation Accuracy: 0.42105263157894735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   4%|▎         | 36/1000 [01:14<33:08,  2.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 80\u001B[0m\n\u001B[0;32m     78\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs, attention_mask\u001B[38;5;241m=\u001B[39mattention_masks, labels\u001B[38;5;241m=\u001B[39mlabels)\n\u001B[0;32m     79\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m---> 80\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     81\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     83\u001B[0m training_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLPProjectVenv\\Lib\\site-packages\\torch\\_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    491\u001B[0m     )\n\u001B[1;32m--> 492\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[0;32m    494\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLPProjectVenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    252\u001B[0m     tensors,\n\u001B[0;32m    253\u001B[0m     grad_tensors_,\n\u001B[0;32m    254\u001B[0m     retain_graph,\n\u001B[0;32m    255\u001B[0m     create_graph,\n\u001B[0;32m    256\u001B[0m     inputs,\n\u001B[0;32m    257\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    258\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    259\u001B[0m )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Tokenize sentences using BERT tokenizer\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Pad sequences and create attention masks\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_LEN = 128\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks = []\n",
    "for sequence in input_ids:\n",
    "    sequence_mask = [float(id > 0) for id in sequence]\n",
    "    attention_masks.append(sequence_mask)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_inputs, validation_inputs, training_labels, validation_labels, training_masks, validation_masks = train_test_split(\n",
    "    input_ids, labels, attention_masks,\n",
    "    random_state=2018, test_size=0.1\n",
    ")\n",
    "\n",
    "# Create DataLoader for training set\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 32\n",
    "training_data = TensorDataset(torch.tensor(training_inputs), torch.tensor(training_masks), torch.tensor(training_labels))\n",
    "training_sampler = RandomSampler(training_data)\n",
    "training_dataloader = DataLoader(training_data, sampler=training_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "validation_data = TensorDataset(torch.tensor(validation_inputs), torch.tensor(validation_masks), torch.tensor(validation_labels))\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "# Configure BERT model for sequence classification\n",
    "from transformers import BertConfig, BertModel\n",
    "configuration = BertConfig()\n",
    "model = BertModel(configuration)\n",
    "configuration = model.config\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                      num_labels=len(label_map.keys()))\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "from transformers import AdamW\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)\n",
    "\n",
    "def flat_accuracy(predicted_labels, labels):\n",
    "    predicted_labels = numpy.argmax(predicted_labels.to('cpu').numpy(), axis=1).flatten()\n",
    "    labels = labels.to('cpu').numpy().flatten()\n",
    "    return numpy.sum(predicted_labels == labels) / len(labels)\n",
    "\n",
    "# Train the BERT model\n",
    "from tqdm import trange\n",
    "epochs = 1000\n",
    "training_losses = []\n",
    "\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    training_steps = 0\n",
    "\n",
    "    for step, batch in enumerate(training_dataloader):\n",
    "        inputs = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss += loss.item()\n",
    "        training_steps += 1\n",
    "\n",
    "        training_losses.append(loss.item())\n",
    "\n",
    "    average_training_loss = training_loss/training_steps\n",
    "    print(\"Epoch {}: Average Training Loss: {}\".format(epoch+1, average_training_loss))\n",
    "\n",
    "    model.eval()\n",
    "    validation_accuracy = 0\n",
    "    validation_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        inputs = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        temp_validation_accuracy = flat_accuracy(logits, labels)\n",
    "        validation_accuracy += temp_validation_accuracy\n",
    "        validation_steps += 1\n",
    "\n",
    "    average_validation_accuracy = validation_accuracy/validation_steps\n",
    "    print(\"Epoch {}: Validation Accuracy: {}\".format(epoch+1, average_validation_accuracy))\n",
    "\n",
    "data_frame = pandas.read_csv(\"out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "sentences = data_frame.sentence.values\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = data_frame.label.values\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks = [[float(i > 0) for i in seq] for seq in input_ids]\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_dataloader = DataLoader(prediction_data, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the BERT model on the out-of-domain dataset\n",
    "model.eval()\n",
    "logits_set = []\n",
    "labels_set = []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "    batch_input_ids, batch_attention_masks, batch_labels = batch\n",
    "    batch_input_ids, batch_attention_masks, batch_labels = batch_input_ids.to(device), batch_attention_masks.to(device), batch_labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_attention_masks)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    logits_set.append(logits.cpu().numpy())\n",
    "    labels_set.append(batch_labels.cpu().numpy())\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_set = []\n",
    "\n",
    "# Calculate Matthews correlation coefficient for each batch\n",
    "for i in range(len(labels_set)):\n",
    "    mcc = matthews_corrcoef(labels_set[i], numpy.argmax(logits_set[i], axis=1).flatten())\n",
    "    matthews_set.append(mcc)\n",
    "\n",
    "for i, mcc in enumerate(matthews_set):\n",
    "    print(f\"Batch {i + 1}: MCC = {mcc}\")\n",
    "\n",
    "# Calculate the overall Matthews correlation coefficient\n",
    "overall_mcc = numpy.mean(matthews_set)\n",
    "print(f\"\\nOverall MCC: {overall_mcc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T01:34:22.587807900Z",
     "start_time": "2024-01-29T01:32:59.591350800Z"
    }
   },
   "id": "890e8b75e5b15fbf",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OUT OF CODE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a537f1023eff8b2f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.892483900Z"
    }
   },
   "id": "862ed469acede438",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "wn.synsets('prophetiqmadshiyn')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.893483700Z"
    }
   },
   "id": "9dede2365cd53679",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_lemmas"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.894484700Z"
    }
   },
   "id": "9de6d1d2cb80beb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('deployed', 'v')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.895484700Z"
    }
   },
   "id": "973dcc9763ee3aec",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_tag"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.896484100Z"
    }
   },
   "id": "de200abb162432",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Watch Cell\n",
    "# print(clean_raw_text(sample_res))\n",
    "# clean_raw_text(sample_res)\n",
    "# nltk.corpus.words.raw().split('\\n')\n",
    "# np.isin(['detail'], nltk.corpus.words.raw().split('\\n'))]\n",
    "def x():\n",
    "    x_dict = nltk.corpus.words.raw().split('\\n')\n",
    "    x_list = [levenshteinDistance('sklearn', word) for word in x_dict]\n",
    "    \n",
    "    id = x_list.index(min(x_list))\n",
    "    print(id)\n",
    "    print(min(x_list))\n",
    "    print(nltk.corpus.words.raw().split('\\n')[id])\n",
    "    \n",
    "x()\n",
    "\n",
    "# lemmatizer.lemmatize('extracting')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.896484100Z"
    }
   },
   "id": "d3a07f1ebda10c20",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import edit_distance as test\n",
    "test('aws', 'reductionqunsobcudt', substitution_cost=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.897483900Z"
    }
   },
   "id": "63d64421478deef8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.898484Z"
    }
   },
   "id": "be3c47f12dd2ff6a",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
