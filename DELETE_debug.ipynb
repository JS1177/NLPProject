{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Classification Task: Initiating Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "630f146353fd6177"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "def generate_class_labels(labels):\n",
    "\n",
    "    unique_labels = list(set(labels))\n",
    "\n",
    "    unique_labels_map = {}\n",
    "\n",
    "    for index, label in enumerate(unique_labels, start=0):\n",
    "        unique_labels_map[label] = index\n",
    "\n",
    "    return unique_labels_map\n",
    "\n",
    "data_frame = pandas.read_csv(\"cleanedResumes.csv\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "data_frame.columns = 'label', 'sentence'\n",
    "\n",
    "labels = data_frame['label']\n",
    "label_map = generate_class_labels(labels)\n",
    "data_frame['label'] = data_frame['label'].apply(lambda label_name: label_map[label_name])\n",
    "\n",
    "data_frame.sample(10)\n",
    "\n",
    "# Prepare sentences and labels\n",
    "sentences = data_frame.sentence.values\n",
    "# sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = data_frame.label.values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T18:08:59.270520300Z",
     "start_time": "2024-01-28T18:08:59.188047600Z"
    }
   },
   "id": "c40657895e78b536",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      label                                           sentence\n0         6  qwtnrvduof education detail may 2013 to may 20...\n1         6  qwtnrvduof area of interest deep learn control...\n2         6  skill r python sap hana tableau sap hana sql s...\n3         6  education detail mca ymcaust faridabad haryana...\n4         6  skill c basic iot python matlab data science m...\n...     ...                                                ...\n9590     24  computer skill proficient in m office word bas...\n9591     24  willingness to accept the challengesbntgbqlmkk...\n9592     24  personal skill quick learner eagerness to lear...\n9593     24  computer skill software knowledge m power poin...\n9594     24  skill set o window xp 7 8 8bntgbqlmkk1 10 data...\n\n[9595 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>qwtnrvduof education detail may 2013 to may 20...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>qwtnrvduof area of interest deep learn control...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>skill r python sap hana tableau sap hana sql s...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>education detail mca ymcaust faridabad haryana...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>skill c basic iot python matlab data science m...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9590</th>\n      <td>24</td>\n      <td>computer skill proficient in m office word bas...</td>\n    </tr>\n    <tr>\n      <th>9591</th>\n      <td>24</td>\n      <td>willingness to accept the challengesbntgbqlmkk...</td>\n    </tr>\n    <tr>\n      <th>9592</th>\n      <td>24</td>\n      <td>personal skill quick learner eagerness to lear...</td>\n    </tr>\n    <tr>\n      <th>9593</th>\n      <td>24</td>\n      <td>computer skill software knowledge m power poin...</td>\n    </tr>\n    <tr>\n      <th>9594</th>\n      <td>24</td>\n      <td>skill set o window xp 7 8 8bntgbqlmkk1 10 data...</td>\n    </tr>\n  </tbody>\n</table>\n<p>9595 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T18:08:59.543058500Z",
     "start_time": "2024-01-28T18:08:59.533860300Z"
    }
   },
   "id": "749d4349796a26ad",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\JS\\anaconda3\\envs\\NLPProjectVenv\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Training Loss: 2.2587072915501065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 1/1000 [00:50<14:01:31, 50.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: 0.6020833333333333\n",
      "Epoch 2: Average Training Loss: 0.43450620615923846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 2/1000 [01:41<14:01:25, 50.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Accuracy: 0.9958333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 2/1000 [02:14<18:39:02, 67.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 80\u001B[0m\n\u001B[0;32m     78\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs, attention_mask\u001B[38;5;241m=\u001B[39mattention_masks, labels\u001B[38;5;241m=\u001B[39mlabels)\n\u001B[0;32m     79\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m---> 80\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     81\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     83\u001B[0m training_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLPProjectVenv\\Lib\\site-packages\\torch\\_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    491\u001B[0m     )\n\u001B[1;32m--> 492\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[0;32m    494\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLPProjectVenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    252\u001B[0m     tensors,\n\u001B[0;32m    253\u001B[0m     grad_tensors_,\n\u001B[0;32m    254\u001B[0m     retain_graph,\n\u001B[0;32m    255\u001B[0m     create_graph,\n\u001B[0;32m    256\u001B[0m     inputs,\n\u001B[0;32m    257\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    258\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    259\u001B[0m )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize sentences using BERT tokenizer\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Pad sequences and create attention masks\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_LEN = 128\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks = []\n",
    "for sequence in input_ids:\n",
    "    sequence_mask = [float(id > 0) for id in sequence]\n",
    "    attention_masks.append(sequence_mask)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_inputs, validation_inputs, training_labels, validation_labels, training_masks, validation_masks = train_test_split(\n",
    "    input_ids, labels, attention_masks,\n",
    "    random_state=2018, test_size=0.1\n",
    ")\n",
    "\n",
    "# Create DataLoader for training set\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 32\n",
    "training_data = TensorDataset(torch.tensor(training_inputs), torch.tensor(training_masks), torch.tensor(training_labels))\n",
    "training_sampler = RandomSampler(training_data)\n",
    "training_dataloader = DataLoader(training_data, sampler=training_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "validation_data = TensorDataset(torch.tensor(validation_inputs), torch.tensor(validation_masks), torch.tensor(validation_labels))\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "# Configure BERT model for sequence classification\n",
    "from transformers import BertConfig, BertModel\n",
    "configuration = BertConfig()\n",
    "model = BertModel(configuration)\n",
    "configuration = model.config\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                      num_labels=len(label_map.keys()))\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "from transformers import AdamW\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)\n",
    "\n",
    "def flat_accuracy(predicted_labels, labels):\n",
    "    predicted_labels = numpy.argmax(predicted_labels.to('cpu').numpy(), axis=1).flatten()\n",
    "    labels = labels.to('cpu').numpy().flatten()\n",
    "    return numpy.sum(predicted_labels == labels) / len(labels)\n",
    "\n",
    "# Train the BERT model\n",
    "from tqdm import trange\n",
    "epochs = 1000\n",
    "training_losses = []\n",
    "\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    training_steps = 0\n",
    "\n",
    "    for step, batch in enumerate(training_dataloader):\n",
    "        inputs = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss += loss.item()\n",
    "        training_steps += 1\n",
    "\n",
    "        training_losses.append(loss.item())\n",
    "\n",
    "    average_training_loss = training_loss/training_steps\n",
    "    print(\"Epoch {}: Average Training Loss: {}\".format(epoch+1, average_training_loss))\n",
    "\n",
    "    model.eval()\n",
    "    validation_accuracy = 0\n",
    "    validation_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        inputs = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        temp_validation_accuracy = flat_accuracy(logits, labels)\n",
    "        validation_accuracy += temp_validation_accuracy\n",
    "        validation_steps += 1\n",
    "\n",
    "    average_validation_accuracy = validation_accuracy/validation_steps\n",
    "    print(\"Epoch {}: Validation Accuracy: {}\".format(epoch+1, average_validation_accuracy))\n",
    "\n",
    "data_frame = pandas.read_csv(\"out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "sentences = data_frame.sentence.values\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = data_frame.label.values\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks = [[float(i > 0) for i in seq] for seq in input_ids]\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_dataloader = DataLoader(prediction_data, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the BERT model on the out-of-domain dataset\n",
    "model.eval()\n",
    "logits_set = []\n",
    "labels_set = []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "    batch_input_ids, batch_attention_masks, batch_labels = batch\n",
    "    batch_input_ids, batch_attention_masks, batch_labels = batch_input_ids.to(device), batch_attention_masks.to(device), batch_labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_attention_masks)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    logits_set.append(logits.cpu().numpy())\n",
    "    labels_set.append(batch_labels.cpu().numpy())\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_set = []\n",
    "\n",
    "# Calculate Matthews correlation coefficient for each batch\n",
    "for i in range(len(labels_set)):\n",
    "    mcc = matthews_corrcoef(labels_set[i], numpy.argmax(logits_set[i], axis=1).flatten())\n",
    "    matthews_set.append(mcc)\n",
    "\n",
    "for i, mcc in enumerate(matthews_set):\n",
    "    print(f\"Batch {i + 1}: MCC = {mcc}\")\n",
    "\n",
    "# Calculate the overall Matthews correlation coefficient\n",
    "overall_mcc = numpy.mean(matthews_set)\n",
    "print(f\"\\nOverall MCC: {overall_mcc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T18:11:55.507001700Z",
     "start_time": "2024-01-28T18:09:00.215637600Z"
    }
   },
   "id": "890e8b75e5b15fbf",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OUT OF CODE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a537f1023eff8b2f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.892483900Z"
    }
   },
   "id": "862ed469acede438",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "wn.synsets('prophetiqmadshiyn')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.893483700Z"
    }
   },
   "id": "9dede2365cd53679",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_lemmas"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.894484700Z"
    }
   },
   "id": "9de6d1d2cb80beb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize('deployed', 'v')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.895484700Z"
    }
   },
   "id": "973dcc9763ee3aec",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_tag"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.896484100Z"
    }
   },
   "id": "de200abb162432",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Watch Cell\n",
    "# print(clean_raw_text(sample_res))\n",
    "# clean_raw_text(sample_res)\n",
    "# nltk.corpus.words.raw().split('\\n')\n",
    "# np.isin(['detail'], nltk.corpus.words.raw().split('\\n'))]\n",
    "def x():\n",
    "    x_dict = nltk.corpus.words.raw().split('\\n')\n",
    "    x_list = [levenshteinDistance('sklearn', word) for word in x_dict]\n",
    "    \n",
    "    id = x_list.index(min(x_list))\n",
    "    print(id)\n",
    "    print(min(x_list))\n",
    "    print(nltk.corpus.words.raw().split('\\n')[id])\n",
    "    \n",
    "x()\n",
    "\n",
    "# lemmatizer.lemmatize('extracting')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.896484100Z"
    }
   },
   "id": "d3a07f1ebda10c20",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import edit_distance as test\n",
    "test('aws', 'reductionqunsobcudt', substitution_cost=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.897483900Z"
    }
   },
   "id": "63d64421478deef8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-28T17:00:47.898484Z"
    }
   },
   "id": "be3c47f12dd2ff6a",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
