{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Context\n",
    "Goal of project is to classify resumes (not to grade resumes). Therefore, goal of preprocessing is to ensure that the text are properly normalized such that they can be properly compared. Priority should therefore be given to keywords etc. due to the specialized nature of each class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "784b56be34640225"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Exploration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42ebd815c363cde7"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.627894500Z",
     "start_time": "2024-02-17T10:24:01.917499900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JS\\AppData\\Local\\Temp\\ipykernel_21464\\4245994788.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics.distance import edit_distance as levenshteinDistance\n",
    "\n",
    "from typing_extensions import Literal"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.716089700Z",
     "start_time": "2024-02-17T10:24:02.627894500Z"
    }
   },
   "id": "26cc9ac6ac95dcab",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "VAR = {\n",
    "    'data_path': os.path.join('UpdatedResumeDataSet_T1_7.csv'),\n",
    "    'batch_size': 32,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.722130700Z",
     "start_time": "2024-02-17T10:24:02.717089900Z"
    }
   },
   "id": "c77f0d91710e6b6e",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "res_data_raw = pd.read_csv(VAR['data_path'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.809861200Z",
     "start_time": "2024-02-17T10:24:02.720131400Z"
    }
   },
   "id": "2fbbfd49f104e2fe",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9595 entries, 0 to 9594\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Category  9595 non-null   object\n",
      " 1   Resume    9595 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 150.1+ KB\n"
     ]
    }
   ],
   "source": [
    "res_data_raw.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.818775600Z",
     "start_time": "2024-02-17T10:24:02.811152300Z"
    }
   },
   "id": "8ccc3c16c91aeae7",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       Category                                             Resume\n0  Data Science  qwtnrvduof Education Details \\nMay 2013 to May...\n1  Data Science  qwtnrvduof Areas of Interest Deep Learning, Co...\n2  Data Science  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...\n3  Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab...\n4  Data Science  SKILLS C Basics, IOT, Python, MATLAB, Data Sci...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Category</th>\n      <th>Resume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Data Science</td>\n      <td>qwtnrvduof Education Details \\nMay 2013 to May...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Data Science</td>\n      <td>qwtnrvduof Areas of Interest Deep Learning, Co...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Data Science</td>\n      <td>Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Data Science</td>\n      <td>Education Details \\r\\n MCA   YMCAUST,  Faridab...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Data Science</td>\n      <td>SKILLS C Basics, IOT, Python, MATLAB, Data Sci...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_data_raw.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.827024900Z",
     "start_time": "2024-02-17T10:24:02.819775200Z"
    }
   },
   "id": "216fb33509475e3",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac225d99793c411"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "9407"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_data_raw['Resume'].duplicated(keep='first').sum() #Check for duplicates"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.828533900Z",
     "start_time": "2024-02-17T10:24:02.825271500Z"
    }
   },
   "id": "209c2fcedb60c5a5",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "res_data = res_data_raw.drop_duplicates(subset=['Resume'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.840043600Z",
     "start_time": "2024-02-17T10:24:02.829533800Z"
    }
   },
   "id": "e4b0cbdd4f0df83f",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Category\nJava Developer               14\nData Science                 12\nHR                           12\nDatabase                     11\nAdvocate                     10\nDotNet Developer              8\nHadoop                        8\nDevOps Engineer               8\nBusiness Analyst              8\nTesting                       8\nCivil Engineer                7\nSAP Developer                 7\nHealth and fitness            7\nPython Developer              7\nArts                          7\nAutomation Testing            7\nElectrical Engineering        6\nSales                         6\nNetwork Security Engineer     6\nETL Developer                 6\nMechanical Engineer           5\nWeb Designing                 5\nBlockchain                    5\nOperations Manager            4\nPMO                           4\nName: count, dtype: int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_data['Category'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.864680700Z",
     "start_time": "2024-02-17T10:24:02.832933100Z"
    }
   },
   "id": "e1195cbd300cb893",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explore Resume Text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41bf6096151b672"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwtnrvduof Education Details \n",
      "May 2013 to May 2017 BbNTGBqLmkKE   UIT-RGPV\n",
      "Data Scientist \n",
      "\n",
      "Data Scientist - Matelabs\n",
      "Skill Details \n",
      "Python- Exprience - Less than 1 year months\n",
      "Statsmodels- Exprience - 12 months\n",
      "AWS- Exprience - Less than 1 year months\n",
      "Machine learning- Exprience - Less than 1 year months\n",
      "Sklearn- Exprience - Less than 1 year months\n",
      "Scipy- Exprience - Less than 1 year months\n",
      "Keras- Exprience - Less than 1 year monthsCompany Details \n",
      "company - Matelabs\n",
      "description - ML Platform for business professionals, dummies and enthusiastsckeKJOFvWQ\n",
      "60/A Koramangala 5th block,\n",
      "Achievements/Tasks behind sukh sagar, Bengaluru,\n",
      "India                               Developed and deployed auto preprocessing steps of machine learning mainly missing value\n",
      "treatment, outlier detection, encoding, scaling, feature selection and dimensionality reductionqunsOBcUdT\n",
      "Deployed automated classification and regression modelRYNOolXhuV\n",
      "linkedinSAJhwmUxoOcom/in/aditya-rathore-\n",
      "b4600b146                           Reasearch and deployed the time series forecasting model ARIMA, SARIMAX, Holt-winter and\n",
      "ProphetiQmADshIYN\n",
      "Worked on meta-feature extracting problemisZoGErzLF\n",
      "githubBRBEGnCeAecom/rathorology\n",
      "Implemented a state of the art research paper on outlier detection for mixed attributes.\n",
      "company - Matelabs\n",
      "description - \n"
     ]
    }
   ],
   "source": [
    "sample_res = res_data['Resume'][0]\n",
    "print(sample_res)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.865680200Z",
     "start_time": "2024-02-17T10:24:02.836141900Z"
    }
   },
   "id": "70ae4ab9b43dc3c5",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initial Review:\n",
    "Some words are seemingly gibberish and consists of a sequence of random characters\n",
    "\n",
    "These words should be removed. However, care must be taken to ensure that other important text such as links are not classified as gibberish\n",
    "\n",
    "List of issues:\n",
    "Broken links (Solved)\n",
    "Long whitespaces (Solved)\n",
    "Combined words without clear separators https://github.com/grantjenks/python-wordsegment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6878125ef3d6a821"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'qwtnrvduof Education Details \\nMay 2013 to May 2017 BbNTGBqLmkKE   UIT-RGPV\\nData Scientist \\n\\nData Scientist - Matelabs\\nSkill Details \\nPython- Exprience - Less than 1 year months\\nStatsmodels- Exprience - 12 months\\nAWS- Exprience - Less than 1 year months\\nMachine learning- Exprience - Less than 1 year months\\nSklearn- Exprience - Less than 1 year months\\nScipy- Exprience - Less than 1 year months\\nKeras- Exprience - Less than 1 year monthsCompany Details \\ncompany - Matelabs\\ndescription - ML Platform for business professionals, dummies and enthusiastsckeKJOFvWQ\\n60/A Koramangala 5th block,\\nAchievements/Tasks behind sukh sagar, Bengaluru,\\nIndia                               Developed and deployed auto preprocessing steps of machine learning mainly missing value\\ntreatment, outlier detection, encoding, scaling, feature selection and dimensionality reductionqunsOBcUdT\\nDeployed automated classification and regression modelRYNOolXhuV\\nlinkedinSAJhwmUxoOcom/in/aditya-rathore-\\nb4600b146                           Reasearch and deployed the time series forecasting model ARIMA, SARIMAX, Holt-winter and\\nProphetiQmADshIYN\\nWorked on meta-feature extracting problemisZoGErzLF\\ngithubBRBEGnCeAecom/rathorology\\nImplemented a state of the art research paper on outlier detection for mixed attributes.\\ncompany - Matelabs\\ndescription - '"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.866680100Z",
     "start_time": "2024-02-17T10:24:02.840043600Z"
    }
   },
   "id": "43c9663538e6da95",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def clean_links(potentialLinks: list):\n",
    "    \n",
    "    '''\n",
    "    Assumption: Potential link will always have at the minimum a .com\n",
    "    \n",
    "    Checks validity of link and returns cleaned link string\n",
    "    '''\n",
    "    \n",
    "    assert isinstance(potentialLinks, list)\n",
    "    \n",
    "    http_exist = False\n",
    "    www_exist = False\n",
    "    com_exist = False\n",
    "    \n",
    "    if len(potentialLinks) < 1:\n",
    "        return []\n",
    "    \n",
    "    ret_list = []\n",
    "    \n",
    "    for link in potentialLinks:\n",
    "        \n",
    "        http_match = re.search(r'(https?)(:)?(\\/){0,2}', link)\n",
    "        www_match = re.search(r'(www)(\\.)?', link)\n",
    "        com_match = re.search(r'(\\.)?(com)', link)\n",
    "        # print('flagged', link)\n",
    "        \n",
    "        #http\n",
    "        if http_match != None:\n",
    "            http_exist = True\n",
    "        \n",
    "        #www\n",
    "        if www_match != None:\n",
    "            www_exist = True\n",
    "        \n",
    "        #com\n",
    "        if com_match != None:\n",
    "            com_exist = True\n",
    "            \n",
    "        if (com_exist) or (com_exist and www_exist) or (com_exist and www_exist and http_exist):\n",
    "            link = re.sub(r'(https?)(:)?(\\/){0,2}', 'https://', link)\n",
    "            link = re.sub(r'(www)(\\.)?', 'www.', link)\n",
    "            link = re.sub(r'(\\.)?(com)', '.com', link)\n",
    "            \n",
    "            ret_list.append(link)\n",
    "        else:\n",
    "            #Not valid link\n",
    "            ret_list.append(False)\n",
    "            \n",
    "    return ret_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.866680100Z",
     "start_time": "2024-02-17T10:24:02.844801800Z"
    }
   },
   "id": "cce23585ea4f68f3",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'qwtnrvduof education details may 2013 to may 2017 bbntgbqlmkke uit rgpv data scientist data scientist matelabs skill details python exprience less than 1 year months statsmodels exprience 12 months aws exprience less than 1 year months machine learning exprience less than 1 year months sklearn exprience less than 1 year months scipy exprience less than 1 year months keras exprience less than 1 year monthscompany details company matelabs description ml platform for business professionals dummies and enthusiastsckekjofvwq 60 a koramangala 5th block achievements tasks behind sukh sagar bengaluru india developed and deployed auto preprocessing steps of machine learning mainly missing value treatment outlier detection encoding scaling feature selection and dimensionality reductionqunsobcudt deployed automated classification and regression modelrynoolxhuv b4600b146 reasearch and deployed the time series forecasting model arima sarimax holt winter and prophetiqmadshiyn worked on meta feature extracting problemiszogerzlf implemented a state of the art research paper on outlier detection for mixed attributes company matelabs description '"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_raw_text(text: str):\n",
    "    \n",
    "    # Clean links section\n",
    "    potential_links = re.findall(\n",
    "        r'(?:(?:https?:?\\/\\/{1,2})?w{1,3}\\.?)?[a-zA-z0-9]{1,2048}\\.?[a-zA-Z0-9]{1,6}\\/\\b[/\\-a-zA-Z0-9]*\\w', text\n",
    "    ) \n",
    "    '''\n",
    "    / will flag a sequence of characters as potential links\n",
    "    \n",
    "    Optional criteions: \n",
    "    http(s)\n",
    "    //\n",
    "    www & .\n",
    "    . & com\n",
    "    '''\n",
    "    \n",
    "    finalized_links = clean_links(potential_links)\n",
    "\n",
    "    for potential_link, finalized_link in zip(potential_links, finalized_links):\n",
    "        if finalized_link == False:\n",
    "            continue\n",
    "        else:\n",
    "#             print('real_links', finalized_link)\n",
    "            text = re.sub(potential_link, ' ', text) #Remove link\n",
    "    \n",
    "    #Clean non-characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', r' ', text)\n",
    "    \n",
    "    #Normalize text\n",
    "    text = text.lower()\n",
    "\n",
    "    #Clean whitespace section\n",
    "    text = re.sub(r'[ ]{1,}', r' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "clean_raw_text(sample_res)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.878553800Z",
     "start_time": "2024-02-17T10:24:02.848487100Z"
    }
   },
   "id": "efc57f3090db2021",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def check_numpy(text):\n",
    "    \n",
    "    if isinstance(text, list):\n",
    "        text = np.array(text)\n",
    "        return text\n",
    "    elif isinstance(text, np.ndarray):\n",
    "        return text\n",
    "    else:\n",
    "        raise TypeError('Not a list or numpy array')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.878553800Z",
     "start_time": "2024-02-17T10:24:02.853817100Z"
    }
   },
   "id": "a8a8d57bd14dd42",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def in_english_corpus(text: list | np.ndarray, behaviour: Literal['inside', 'outside'] = 'inside'):\n",
    "    \n",
    "    text = check_numpy(text)\n",
    "\n",
    "    english_dictionary = nltk.corpus.words.raw().split('\\n')\n",
    "\n",
    "    english_dictionary = [word.lower() for word in english_dictionary] # normalize to lowercase\n",
    "    \n",
    "    word_in_dict_bool = np.isin(text, english_dictionary)\n",
    "    \n",
    "    if behaviour == 'inside':\n",
    "        return text[word_in_dict_bool]\n",
    "    elif behaviour == 'outside':\n",
    "        word_not_in_dict_bool = np.invert(word_in_dict_bool)\n",
    "        return text[word_not_in_dict_bool]\n",
    "    else:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.878553800Z",
     "start_time": "2024-02-17T10:24:02.857189100Z"
    }
   },
   "id": "837075b2fbc9d445",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def clean_structured_text(text: list | np.ndarray, customer_dictionary: list = nltk.corpus.words.raw().split('\\n')):\n",
    "    \n",
    "    #TODO There may be no point to cleaning mistyped random words > Intefere with keywords > Model may have to simply learn the noise\n",
    "\n",
    "    text = check_numpy(text)\n",
    "    \n",
    "    customer_dictionary = [word.lower() for word in customer_dictionary] # normalize to lowercase\n",
    "    \n",
    "    word_in_dict_bool = np.isin(text, customer_dictionary)\n",
    "    \n",
    "    word_not_in_dict_bool = np.invert(word_in_dict_bool)\n",
    "    \n",
    "    \n",
    "    \n",
    "    words_in_dict = text[word_not_in_dict_bool]\n",
    "    \n",
    "    print(words_in_dict)\n",
    "\n",
    "# clean_structured_text(sample_lemmas)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.941609200Z",
     "start_time": "2024-02-17T10:24:02.861680300Z"
    }
   },
   "id": "d48354035fc76fb",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def wordnet_tag_format(tag: str):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if tag.startswith('A'):\n",
    "        return 'a'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    \n",
    "    return 'n' #Ensure lemmatize function can run"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.945610300Z",
     "start_time": "2024-02-17T10:24:02.874604Z"
    }
   },
   "id": "60254b9a74b9fe2c",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_lemmas(tagged_tokens: list[tuple], lemmatizer=nltk.stem.WordNetLemmatizer()):\n",
    "    lemmas = [lemmatizer.lemmatize(token[0], wordnet_tag_format(token[1])) for token in tagged_tokens]\n",
    "    \n",
    "    return lemmas"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.946610700Z",
     "start_time": "2024-02-17T10:24:02.879553700Z"
    }
   },
   "id": "bab679f05de30967",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "--- Start Analysis ---\n",
    "### Goal:\n",
    "\n",
    "With reference to a list of possible english words, we aim to separate keywords important to job scopes and misspelled/invalid words. With the list of misspelled/invalids words, we can find the closest possible related word using Levenshtein Distance.\n",
    "\n",
    "### Theory:\n",
    "\n",
    "Since most misspelled words with random number of combinations occuringly more than once has a very small probability it is more likely that we will see keywords occur more frequently compared to misspelled words and characters of random sequence."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a58b353972d45cb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_common_words_from_raw_data_ood(resumes_df: pd.DataFrame, column: str):\n",
    "    \n",
    "    resumes = resumes_df[column].to_numpy()\n",
    "    resumes = check_numpy(resumes)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    counter = Counter()\n",
    "    \n",
    "    for index, resume in enumerate(resumes):\n",
    "        normalized_resume = extract_lemmas(\n",
    "            nltk.pos_tag(\n",
    "                nltk.tokenize.word_tokenize(\n",
    "                    clean_raw_text(resume))), \n",
    "            lemmatizer)\n",
    "        \n",
    "        # out of dictionary\n",
    "        ood = in_english_corpus(normalized_resume, 'outside')\n",
    "        counter.update(ood)\n",
    "        \n",
    "        if index % 10 == 0:\n",
    "            print(index)\n",
    "        \n",
    "    return counter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.949610Z",
     "start_time": "2024-02-17T10:24:02.883853500Z"
    }
   },
   "id": "61ec85bbd1682100",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# count = extract_common_words_from_raw_data_ood(res_data, 'Resume')\n",
    "# print(count)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.949610Z",
     "start_time": "2024-02-17T10:24:02.886143700Z"
    }
   },
   "id": "a3fca6554ace1bbb",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Output Analysis\n",
    "\n",
    "Clearly misspelled words like \"exprience\" occur most frequently, and sequence of seemingly random characters \"bbntgbqlmkkeckekjofvwq\" appeared 5 times.\n",
    "\n",
    "Conversely, keywords like \"mozilla\" which may be important to Web Designers only appeared once. Other important keywords like \"tensorflow\" and \"scikit\" only appears 5 times, the same as \"bbntgbqlmkkeckekjofvwq\". I therefore hypothesise that words such as \"bbntgbqlmkkeckekjofvwq\" occurring is not based on chance due to the miniscule probability. There is thus no apparent clear threshold/boundary between misspelled/noise words and keywords.\n",
    "\n",
    "--- End Analysis ---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db87d52b2223b323"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pipeline(filepath: str, feature_name: str):\n",
    "    \n",
    "    def total_normalize(text):\n",
    "        \n",
    "        text = clean_raw_text(text)\n",
    "        \n",
    "        # # Stopword Removal\n",
    "        text = ' '.join([word for word in text.split() if word not in (stopwords.words('english'))])\n",
    "        \n",
    "        # Lemmatization\n",
    "        text_tag = nltk.pos_tag(\n",
    "            nltk.word_tokenize(text)\n",
    "        )\n",
    "        text_lemmas = extract_lemmas(text_tag)\n",
    "        \n",
    "        return ' '.join(text_lemmas)\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df.drop_duplicates(subset=[feature_name], keep='first')\n",
    "    df[feature_name] = df[feature_name].apply(total_normalize)\n",
    "    \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:02.949610Z",
     "start_time": "2024-02-17T10:24:02.891056600Z"
    }
   },
   "id": "69961c6ec093dd8c",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'skill r python sap hana tableau sap hana sql sap hana pal m sql sap lumira c linear program data model advance analytics scm analytics retail analytics social medium analytics nlp education detail january 2017 january 2018 pgdm business analytics great lake institute management illinois institute technology january 2013 bachelor engineering electronics communication bengaluru karnataka new horizon college engineering bangalore visvesvaraya technological university data science consultant consultant deloitte usi skill detail linear program exprience 6 month retail exprience 6 month retail marketing exprience 6 month scm exprience 6 month sql exprience less 1 year month deep learn exprience le 1 year month machine learn exprience le 1 year month python exprience le 1 year month r exprience le 1 year monthscompany detail company deloitte usi description project involve analyse historic deal come insight optimize future dealsbntgbqlmkk role give raw data carry end end analysis present insight clientckekjofvwq key responsibility extract data client system across geographiesqunsobcudt understand build report tableaurynoolxhuv infer meaningful insight optimize price find process blockadessajhwmuxoo technical environment r tableauiqmadshiyn industry cross industry service area cross industry product project name handwrite recognition consultant 3 monthsiszogerzlf project involve take handwritten image convert digital text image object detection sentence creationbrbegnceae role develop sentence correction functionality key responsibility gather data large enough capture english word train lstm model word technical environment python industry finance service area financial service bi development project name swift consultant 8 month project develop analytics infrastructure top sap 4 would user view financial report respective department report also include forecasting expense role lead offshore team key responsibility design develop data model report develop etl data flow validate various report technical environment sap hana tableau sap ao industry healthcare analytics service area life science product development project name clinical healthcare system consultant 2 month project develop analytics infrastructure top argus would allow user query faster provide advance analytics capability role involve design deploy phase perform lot data restructure build model insight key responsibility design develop data model report develop deploy analytical model validate various report technical environment data model sap hana tableau nlp industry fmcg service area trade promotion project name consumption base plan flower food consultant 8 month project involve set crm cbp module role involve key data decomposition activity set base future year forecast course project develop various model carry key performance improvement key responsibility design develop hana model decomposition develop data flow forecast develop various view report customer sale fund validate various report bobj technical environment data model sap hana bobj time series forecast internal initiative industry fmcg customer segmentation rfm analysis consultant 3 month initiative involved set hana python interface advance analytics python course successfully segment data five core segment use k mean carry rfm analysis python also develop algorithm categorize new customer define bucket technical environment anaconda3 python3 6 hana sps12 industry telecom invoice state detection consultant 1 month initiative reduce manual effort verify close open invoice manually involve development decision tree classify open close invoice enable effort reduction 60 technical environment r sap pal sap hana sps12 accenture experience industry analytics cross industry process analytics sap senior developer 19 month accenture solution pvt ltd india project involve development sap analytics tool process analytics ipa role develop database object data model provide operational insight client role develop various finance relate kpis spearhead various deployment introduce sap predictive analytics reduce development time reuse functionality kpis prepared production planning report key responsibility involve information gather phase design implement sap hana data model use attribute view analytic view calculation view develop various kpi individually use complex sql script calculation view create procedure hana database take ownership developed dashboard functionality involve building data processing algorithm execute r server cluster analysis technical environment r sap hana sql industry cross industry accenture test accelerator sap database developer 21 month accenture solution pvt ltd india role take care development activity atas tool also complete various deployment product apart activity also actively involved maintenance database server production quality key responsibility analyze business requirement understand scope get requirement clarify interact business transform requirement generate attribute mapping document review map specification documentation create update database object like table view store procedure function package monitor sql server error log application log sql server agent prepared data flow diagram entity relationship diagram use uml responsible design develop normalization database table experience performance tune use sql profiler involve qa uat knowledge transfer support activity technical environment sql server 2008 2014 visual studio 2010 windows server performance monitor sql server profiler c pl sql sql'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_resumes = pipeline('UpdatedResumeDataSet_T1_7.csv', feature_name='Resume')\n",
    "processed_resumes['Resume'][2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:11.011189500Z",
     "start_time": "2024-02-17T10:24:02.894004700Z"
    }
   },
   "id": "2b9992be2a73bbda",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "processed_resumes.to_csv('cleanedResumes.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:11.023496400Z",
     "start_time": "2024-02-17T10:24:11.012189500Z"
    }
   },
   "id": "3243fd5c5ea632ba",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OUT OF CODE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a537f1023eff8b2f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105091\n",
      "2\n",
      "learn\n"
     ]
    }
   ],
   "source": [
    "# Watch Cell\n",
    "# print(clean_raw_text(sample_res))\n",
    "# clean_raw_text(sample_res)\n",
    "# nltk.corpus.words.raw().split('\\n')\n",
    "# np.isin(['detail'], nltk.corpus.words.raw().split('\\n'))]\n",
    "def x():\n",
    "    x_dict = nltk.corpus.words.raw().split('\\n')\n",
    "    x_list = [levenshteinDistance('sklearn', word) for word in x_dict]\n",
    "    \n",
    "    id = x_list.index(min(x_list))\n",
    "    print(id)\n",
    "    print(min(x_list))\n",
    "    print(nltk.corpus.words.raw().split('\\n')[id])\n",
    "    \n",
    "x()\n",
    "\n",
    "# lemmatizer.lemmatize('extracting')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T10:24:18.200626300Z",
     "start_time": "2024-02-17T10:24:11.021495800Z"
    }
   },
   "id": "d3a07f1ebda10c20",
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
