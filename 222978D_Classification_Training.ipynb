{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_class_labels(labels):\n",
    "\n",
    "    unique_labels = list(set(labels))\n",
    "\n",
    "    unique_labels_map = {}\n",
    "\n",
    "    for index, label in enumerate(unique_labels, start=0):\n",
    "        unique_labels_map[label] = index\n",
    "\n",
    "    return unique_labels_map"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T02:49:39.671761700Z",
     "start_time": "2024-01-29T02:49:39.667959400Z"
    }
   },
   "id": "262d03d537bb9d07",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "VAR = {\n",
    "    'max_len': 128,\n",
    "    'batch_size': 32\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T02:49:39.686007Z",
     "start_time": "2024-01-29T02:49:39.674090900Z"
    }
   },
   "id": "bfd2aed3b089c3cd",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jscho\\AppData\\Local\\Temp\\ipykernel_15624\\1128236990.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jscho\\anaconda3\\envs\\NLPProjectVenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, AdamW, BertForSequenceClassification\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T02:49:47.337287500Z",
     "start_time": "2024-01-29T02:49:39.677670600Z"
    }
   },
   "id": "5754cad4179ffa39",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification Task: Initiating Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bdcc74464b9a543"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\jscho\\anaconda3\\envs\\NLPProjectVenv\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Training Loss: 2.259314641498384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 1/4 [01:25<04:17, 85.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: 0.7499519969278035\n",
      "Epoch 2: Average Training Loss: 0.36996116613348323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 2/4 [03:07<03:10, 95.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Accuracy: 0.996031746031746\n",
      "Epoch 3: Average Training Loss: 0.0586814710604293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 3/4 [06:13<02:16, 136.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Validation Accuracy: 0.9990079365079365\n",
      "Epoch 4: Average Training Loss: 0.08461866758409001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [09:06<00:00, 136.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Validation Accuracy: 0.9945436507936508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "resume_df = pandas.read_csv(\"cleanedResumes.csv\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "labels = resume_df['Category']\n",
    "label_map = generate_class_labels(labels)\n",
    "\n",
    "resume_df['Category'] = resume_df['Category'].apply(lambda label_name: label_map[label_name])\n",
    "\n",
    "resumes = resume_df.Resume.values\n",
    "labels = resume_df.Category.values\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = [tokenizer.tokenize(resume) for resume in resumes]\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=VAR['max_len'], dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks = []\n",
    "for sequence in input_ids:\n",
    "    sequence_mask = [float(id > 0) for id in sequence]\n",
    "    attention_masks.append(sequence_mask)\n",
    "\n",
    "training_inputs, validation_test_inputs, training_labels, validation_test_labels, training_masks, validation_test_masks = train_test_split(\n",
    "    input_ids, labels, attention_masks,\n",
    "    random_state=42, test_size=0.3\n",
    ")\n",
    "\n",
    "validation_inputs, testing_inputs, validation_labels, testing_labels, validation_masks, testing_masks = train_test_split(\n",
    "    validation_test_inputs, validation_test_labels, validation_test_masks, random_state=42, test_size=0.3\n",
    ")\n",
    "\n",
    "training_data = TensorDataset(torch.tensor(training_inputs), torch.tensor(training_masks), torch.tensor(training_labels))\n",
    "training_sampler = RandomSampler(training_data)\n",
    "training_dataloader = DataLoader(training_data, sampler=training_sampler, batch_size=VAR['batch_size'])\n",
    "\n",
    "validation_data = TensorDataset(torch.tensor(validation_inputs), torch.tensor(validation_masks), torch.tensor(validation_labels))\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=VAR['batch_size'])\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "configuration = BertConfig()\n",
    "model = BertModel(configuration)\n",
    "configuration = model.config\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_map.keys()))\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)\n",
    "\n",
    "def flat_accuracy(predicted_labels, labels):\n",
    "    predicted_labels = numpy.argmax(predicted_labels.to('cpu').numpy(), axis=1).flatten()\n",
    "    labels = labels.to('cpu').numpy().flatten()\n",
    "    return numpy.sum(predicted_labels == labels) / len(labels)\n",
    "\n",
    "from tqdm import trange\n",
    "epochs = 4\n",
    "training_losses = []\n",
    "\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    training_steps = 0\n",
    "\n",
    "    for step, batch in enumerate(training_dataloader):\n",
    "        inputs = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss += loss.item()\n",
    "        training_steps += 1\n",
    "\n",
    "        training_losses.append(loss.item())\n",
    "\n",
    "    average_training_loss = training_loss/training_steps\n",
    "    print(\"Epoch {}: Average Training Loss: {}\".format(epoch+1, average_training_loss))\n",
    "\n",
    "    model.eval()\n",
    "    validation_accuracy = 0\n",
    "    validation_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        inputs = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        temp_validation_accuracy = flat_accuracy(logits, labels)\n",
    "        validation_accuracy += temp_validation_accuracy\n",
    "        validation_steps += 1\n",
    "\n",
    "    average_validation_accuracy = validation_accuracy/validation_steps\n",
    "    print(\"Epoch {}: Validation Accuracy: {}\".format(epoch+1, average_validation_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T02:59:53.998006200Z",
     "start_time": "2024-01-29T02:49:47.348790100Z"
    }
   },
   "id": "4f600ad255b3cc31",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import random\n",
    "# \n",
    "# minimum = 7\n",
    "# current_lowest = data_frame['Category'].value_counts().min()\n",
    "# \n",
    "# #Check\n",
    "# count = data_frame['Category'].value_counts()\n",
    "# remaining = 7 - count[count<minimum]\n",
    "# \n",
    "# while len(remaining != 0):\n",
    "#     count = data_frame['Category'].value_counts()\n",
    "#     remaining = 7 - count[count<minimum]\n",
    "# \n",
    "#     for category in remaining.index:\n",
    "#         someInt = random.randint(0, current_lowest-1)\n",
    "#         value_to_append = processed_resumes[\n",
    "#             processed_resumes['Category']==category\n",
    "#         ]['Resume'].values[someInt]\n",
    "# \n",
    "# \n",
    "#         df_to_concat = pandas.DataFrame({\n",
    "#             'Category': [category],\n",
    "#             'Resume': [value_to_append]\n",
    "#         })\n",
    "# \n",
    "#         processed_resumes = pandas.concat([processed_resumes, df_to_concat], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-29T02:59:53.988620400Z"
    }
   },
   "id": "76dcee7c9f3e927e",
   "execution_count": 0
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
